import cv2
import torch
import numpy as np
from torchvision import transforms
import torch.nn as nn
from torchvision import models
import dlib
import mediapipe as mp

# -------------------
# 1ï¸âƒ£ ì„¤ì •
# -------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

PREDICTOR_PATH = "/Users/hanjiho/Desktop/eye detect/eye_blink_detector-master/face recognition/shape_predictor_68_face_landmarks.dat"
SAVE_PATH = "/Users/hanjiho/Desktop/eye detect/eye_blink_detector-master/face recognition/best_multitask_model.pth"

EYE_CROP_PADDING = 20

# -------------------
# 2ï¸âƒ£ dlib + MediaPipe ì´ˆê¸°í™”
# -------------------
detector = dlib.get_frontal_face_detector()
try:
    predictor = dlib.shape_predictor(PREDICTOR_PATH)
except RuntimeError as e:
    print(f"dlib predictor ë¡œë“œ ì˜¤ë¥˜: {e}")
    exit()

mp_face = mp.solutions.face_mesh.FaceMesh(
    static_image_mode=False, 
    max_num_faces=1, 
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
    refine_landmarks=True # ë™ê³µ ì¢Œí‘œ(473)ë¥¼ ìœ„í•´ True
)

# -------------------
# 3ï¸âƒ£ Multimodal Model (í›ˆë ¨ ì½”ë“œ(R20)ì™€ ë™ì¼)
# -------------------
class MultiModalBlinkModel(nn.Module):
    def __init__(self, img_dim=512, feature_dim=68*2 + 468*3): # 1540
        super().__init__()
        base_model = models.resnet18(pretrained=False) 
        base_model.fc = nn.Identity()
        self.cnn = base_model 

        self.feature_fc = nn.Sequential(
            nn.Linear(feature_dim, 512), # 1540 -> 512
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        self.blink_fc = nn.Sequential(
            nn.Linear(512 + img_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 2)
        )

    def forward(self, img, features):
        img_feat = self.cnn(img)
        feat = self.feature_fc(features)
        combined = torch.cat((img_feat, feat), dim=1)
        blink_out = self.blink_fc(combined)
        return blink_out

# -------------------
# 4ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
# -------------------
feature_dim = 68*2 + 468*3 # 1540
model = MultiModalBlinkModel(feature_dim=feature_dim).to(DEVICE)
model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
model.eval() 
print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {SAVE_PATH}")

# -------------------
# 5ï¸âƒ£ Transform ì •ì˜
# -------------------
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
])

# -------------------
# ğŸ”´ NEW: 5.5ï¸âƒ£ Head Pose (PnP) ì„¤ì •ì„ ìœ„í•œ ë³€ìˆ˜
# -------------------
# PnPë¥¼ ìœ„í•œ 3D ì–¼êµ´ ëª¨ë¸ í¬ì¸íŠ¸ (MediaPipe ëœë“œë§ˆí¬ ê¸°ì¤€)
# ìŠ¤ì¼€ì¼ì€ ì¤‘ìš”í•˜ì§€ ì•Šìœ¼ë©°, ìƒëŒ€ì ì¸ ìœ„ì¹˜ê°€ ì¤‘ìš”í•¨.
model_points = np.array([
    (0.0, 0.0, 0.0),             # 1. ì½” ë (Nose tip)
    (0.0, -330.0, -65.0),        # 152. í„± (Chin)
    (-225.0, 170.0, -135.0),     # 33. ì™¼ìª½ ëˆˆ ì™¼ìª½ ë (Left eye left corner)
    (225.0, 170.0, -135.0),      # 263. ì˜¤ë¥¸ìª½ ëˆˆ ì˜¤ë¥¸ìª½ ë (Right eye right corner)
    (-150.0, -150.0, -125.0),    # 61. ì™¼ìª½ ì… ë (Left mouth corner)
    (150.0, -150.0, -125.0)      # 291. ì˜¤ë¥¸ìª½ ì… ë (Right mouth corner)
])

# ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤ (ì›¹ìº  í¬ê¸°ì— ë”°ë¼ ë£¨í”„ ì§„ì… ì „ ì„¤ì •)
camera_matrix = np.zeros((3,3))
dist_coeffs = np.zeros((4, 1)) # ë Œì¦ˆ ì™œê³¡ ì—†ë‹¤ê³  ê°€ì •

# -------------------
# 6ï¸âƒ£ ì›¹ìº  ì‹¤í–‰
# -------------------
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("ì˜¤ë¥˜: ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# ğŸ”´ NEW: PnPë¥¼ ìœ„í•´ í”„ë ˆì„ í¬ê¸°ë¥¼ ë¨¼ì € ì½ì–´ì™€ì„œ ì¹´ë©”ë¼ ë§¤íŠ¸ë¦­ìŠ¤ ì„¤ì •
ret, frame = cap.read()
if not ret:
    print("ì˜¤ë¥˜: ì›¹ìº ì—ì„œ ì²« í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    cap.release()
    exit()
    
h, w, _ = frame.shape
FOCAL_LENGTH_ESTIMATE = w # ê°„ë‹¨í•œ ì¶”ì • (ì¼ë°˜ì ìœ¼ë¡œ wì™€ ë¹„ìŠ·)
camera_matrix = np.array([
    [FOCAL_LENGTH_ESTIMATE, 0, w / 2],
    [0, FOCAL_LENGTH_ESTIMATE, h / 2],
    [0, 0, 1]
], dtype="double")
print(f"Frame (h, w) = ({h}, {w}). PnPìš© Camera matrix ì´ˆê¸°í™” ì™„ë£Œ.")


blink_flag = False
blink_count = 0

print("ì›¹ìº  ì‹¤í–‰ ì¤‘... (ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤)")

while True:
    ret, frame = cap.read()
    if not ret:
        print("ì˜¤ë¥˜: í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break
    
    frame = cv2.flip(frame, 1) 
    
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    results = mp_face.process(rgb_frame)
    rects = detector(gray_frame)
    
    face_detected = False
    gaze_text = "N/A" # ì‹œì„  ê¸°ë³¸ê°’ (ë¬¸ìì—´ë¡œ ìœ ì§€)
    
    # ğŸ”´ NEW: í—¤ë“œ í¬ì¦ˆ ë³€ìˆ˜ ì´ˆê¸°í™”
    head_pitch = 0.0
    head_yaw = 0.0
    
    img_tensor = torch.zeros((1, 3, 224, 224), dtype=torch.float32).to(DEVICE)
    features_tensor = torch.zeros((1, feature_dim), dtype=torch.float32).to(DEVICE)
    
    if len(rects) > 0:
        face_detected = True
        shape = predictor(gray_frame, rects[0])
        
        # --- 1. ëª¨ë¸ ì…ë ¥ (ì´ë¯¸ì§€) & ëˆˆ ìœ„ì¹˜ ì‹œê°í™” (dlib ê¸°ë°˜) ---
        eye_coords_x = []
        eye_coords_y = []
        for i in range(36, 48):
            eye_coords_x.append(shape.part(i).x)
            eye_coords_y.append(shape.part(i).y)
        
        x_min = max(0, min(eye_coords_x) - EYE_CROP_PADDING)
        x_max = min(frame.shape[1], max(eye_coords_x) + EYE_CROP_PADDING)
        y_min = max(0, min(eye_coords_y) - EYE_CROP_PADDING)
        y_max = min(frame.shape[0], max(eye_coords_y) + EYE_CROP_PADDING)
        
        if x_max > x_min and y_max > y_min:
            eye_crop_img = frame[y_min:y_max, x_min:x_max]
            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 255, 0), 2)
            rgb_crop = cv2.cvtColor(eye_crop_img, cv2.COLOR_BGR2RGB)
            img_tensor = transform(rgb_crop).unsqueeze(0).to(DEVICE)
        else:
            face_detected = False

        # --- 2. ëª¨ë¸ ì…ë ¥ (íŠ¹ì§•) ì¤€ë¹„ (dlib + MediaPipe) ---
        # h, w ëŠ” ë£¨í”„ ë°–ì—ì„œ ì´ë¯¸ ì •ì˜ë¨
        dlib_coords = []
        for p in shape.parts():
            dlib_coords.extend([p.x / w, p.y / h]) 
        dlib_f = np.array(dlib_coords)

        mp_f = np.zeros(468 * 3)
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark 
            mp_coords = []
            
            # ğŸ”´ NEW: Head Pose (PnP) ê³„ì‚° (MediaPipe ëœë“œë§ˆí¬ ì‚¬ìš©)
            try:
                # PnPì— ì‚¬ìš©í•  2D ì´ë¯¸ì§€ í¬ì¸íŠ¸
                image_points = np.array([
                    (landmarks[1].x * w, landmarks[1].y * h),    # 1. Nose
                    (landmarks[152].x * w, landmarks[152].y * h), # 152. Chin
                    (landmarks[33].x * w, landmarks[33].y * h),   # 33. Left eye corner
                    (landmarks[263].x * w, landmarks[263].y * h), # 263. Right eye corner
                    (landmarks[61].x * w, landmarks[61].y * h),   # 61. Left mouth corner
                    (landmarks[291].x * w, landmarks[291].y * h)  # 291. Right mouth corner
                ], dtype="double")
                
                (success, rotation_vector, translation_vector) = cv2.solvePnP(
                    model_points, 
                    image_points, 
                    camera_matrix, 
                    dist_coeffs,
                    flags=cv2.SOLVEPNP_ITERATIVE # (cv2.SOLVEPNP_SQPNP or cv2.SOLVEPNP_ITERATIVE)
                )
                
                # íšŒì „ ë²¡í„°ë¥¼ Euler ê°ë„ë¡œ ë³€í™˜ (Yaw, Pitch, Roll)
                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
                
                sy = np.sqrt(rotation_matrix[0, 0]**2 + rotation_matrix[1, 0]**2)
                singular = sy < 1e-6
                
                if not singular:
                    head_pitch = np.arctan2(-rotation_matrix[2, 0], sy) * 180 / np.pi
                    head_yaw = np.arctan2(rotation_matrix[1, 0], rotation_matrix[0, 0]) * 180 / np.pi
                    # head_roll = np.arctan2(rotation_matrix[2, 1], rotation_matrix[2, 2]) * 180 / np.pi
                else:
                    head_pitch = np.arctan2(-rotation_matrix[2, 0], sy) * 180 / np.pi
                    head_yaw = np.arctan2(-rotation_matrix[0, 1], rotation_matrix[1, 1]) * 180 / np.pi
                    # head_roll = 0.0
                
                # ğŸ”´ NEW: PnP ê²°ê³¼ (ì–¼êµ´ ë°©í–¥) ì‹œê°í™” (ë³´ë¼ìƒ‰ ì„ )
                (nose_end_point2D, _) = cv2.projectPoints(
                    np.array([(0.0, 0.0, 500.0)]), # ì½” ë(0,0,0)ì—ì„œ Zì¶•(ì •ë©´)ìœ¼ë¡œ 500mm
                    rotation_vector, 
                    translation_vector, 
                    camera_matrix, 
                    dist_coeffs
                )
                p1 = (int(image_points[0][0]), int(image_points[0][1])) # ì½” ë
                p2 = (int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))
                cv2.arrowedLine(frame, p1, p2, (255, 0, 255), 3) # ë³´ë¼ìƒ‰
                    
            except Exception as e:
                # print(f"Head pose PnP error: {e}")
                head_pitch, head_yaw = 0.0, 0.0
            
            # (ê¸°ì¡´ ì½”ë“œ) íŠ¹ì§• ë²¡í„° ì¤€ë¹„
            for i in range(468):
                lm = landmarks[i]
                mp_coords.extend([lm.x, lm.y, lm.z])
            mp_f = np.array(mp_coords)

            # --- ğŸ”´ 3. ì‹œì„  ì¶”ì • (MediaPipe ë™ê³µ) - ìˆ«ì ë§¤í•‘ (ê¸°ì¡´ ìœ ì§€) ---
            try:
                outer_corner_x = landmarks[33].x
                inner_corner_x = landmarks[133].x
                pupil_x = landmarks[473].x
                
                eye_width = abs(inner_corner_x - outer_corner_x)
                if eye_width > 0: 
                    pupil_pos = (pupil_x - outer_corner_x) / eye_width
                    gaze_value = (pupil_pos - 0.5) * 2.0
                    gaze_text = f"{gaze_value:.2f}"
                    
            except Exception as e:
                gaze_text = "Error"
                
        features = np.concatenate([dlib_f, mp_f]) 
        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        
    else:
        face_detected = False

    # --- 4. ëª¨ë¸ ì¶”ë¡  (ê¹œë¹¡ì„) ---
    pred = 1 
    if face_detected:
        with torch.no_grad():
            blink_out = model(img_tensor, features_tensor)
            pred = torch.argmax(blink_out, dim=1).item() 
    else:
        gaze_text = "N/A" 

    # --- 5. ê¹œë¹¡ì„ ì¹´ìš´íŠ¸ ---
    if pred == 0: 
        if not blink_flag:
            blink_flag = True
    else: 
        if blink_flag:
            blink_flag = False
            blink_count += 1
            # print(f"Blink! (Total: {blink_count})") # ì½˜ì†” ì¶œë ¥ ì¤„ì„

    # --- 6. í™”ë©´ í‘œì‹œ (ìˆ˜ì •ë¨) ---
    if not face_detected:
        status_text = "Face Not Detected"
        status_color = (0, 0, 255)
        gaze_text = "N/A"
        head_yaw, head_pitch = 0.0, 0.0 # N/Aë¡œ í‘œì‹œë˜ë„ë¡ ë¦¬ì…‹
    else:
        status_text = "Closed" if pred == 0 else "Open"
        status_color = (0, 0, 255) if pred == 0 else (0, 255, 0)
    
    cv2.putText(frame, f"Status: {status_text}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)
    cv2.putText(frame, f"Blink Count: {blink_count}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
    
    # ### ğŸ”´ NEW: ì‹œì„  í…ìŠ¤íŠ¸ + í—¤ë“œ í¬ì¦ˆ ê°ë„ í‘œì‹œ ###
    gaze_display_color = (128, 128, 128) # ê¸°ë³¸ê°’ (íšŒìƒ‰)
    
    if pred == 1 and face_detected: 
        # ëˆˆì„ ë–´ê³  ì–¼êµ´ì´ ê°ì§€ë˜ì—ˆì„ ë•Œë§Œ ì‹œì„ /ê°ë„ í‘œì‹œ
        gaze_display_color = (0, 255, 255) # ë…¸ë€ìƒ‰
        
        # 1. ë™ê³µ ê¸°ì¤€ (ìƒëŒ€ ìœ„ì¹˜)
        cv2.putText(frame, f"Pupil Pos: {gaze_text}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)
        
        # 2. ì–¼êµ´ ê°ë„ (PnP)
        # Yaw (ì¢Œìš°), Pitch (ìƒí•˜)
        cv2.putText(frame, f"Head Yaw (L/R): {head_yaw:.1f} deg", (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)
        cv2.putText(frame, f"Head Pitch (U/D): {head_pitch:.1f} deg", (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)
    
    else:
        # ì–¼êµ´ì´ ì—†ê±°ë‚˜ ëˆˆì„ ê°ì•˜ì„ ë•Œ
        cv2.putText(frame, f"Pupil Pos: N/A", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)
        cv2.putText(frame, f"Head Yaw (L/R): N/A", (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)
        cv2.putText(frame, f"Head Pitch (U/D): N/A", (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.7, gaze_display_color, 2)


    cv2.imshow("Blink Detection + Gaze (ESC to exit)", frame)

    if cv2.waitKey(1) & 0xFF == 27:  # ESC í‚¤
        break

# -------------------
# 7ï¸âƒ£ ì¢…ë£Œ
# -------------------
cap.release()
mp_face.close()
cv2.destroyAllWindows()
print(f"Final Blink Count: {blink_count}")
