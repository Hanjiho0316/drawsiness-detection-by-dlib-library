import cv2
import torch
import numpy as np
from torchvision import transforms
import torch.nn as nn
from torchvision import models
import dlib
import mediapipe as mp

# -------------------
# 1ï¸âƒ£ ì„¤ì •
# -------------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

PREDICTOR_PATH = "/Users/hanjiho/Desktop/eye detect/eye_blink_detector-master/face recognition/shape_predictor_68_face_landmarks.dat"
SAVE_PATH = "/Users/hanjiho/Desktop/eye detect/eye_blink_detector-master/face recognition/best_multitask_model.pth"

EYE_CROP_PADDING = 20
# (ì„ê³„ê°’ ëŒ€ì‹  ìˆ«ì ë§¤í•‘ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì„ê³„ê°’ ì„¤ì • ì œê±°)
# GAZE_THRESHOLD_LEFT = 0.4
# GAZE_THRESHOLD_RIGHT = 0.6

# -------------------
# 2ï¸âƒ£ dlib + MediaPipe ì´ˆê¸°í™”
# -------------------
detector = dlib.get_frontal_face_detector()
try:
    predictor = dlib.shape_predictor(PREDICTOR_PATH)
except RuntimeError as e:
    print(f"dlib predictor ë¡œë“œ ì˜¤ë¥˜: {e}")
    exit()

mp_face = mp.solutions.face_mesh.FaceMesh(
    static_image_mode=False, 
    max_num_faces=1, 
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
    refine_landmarks=True # ë™ê³µ ì¢Œí‘œ(473)ë¥¼ ìœ„í•´ True
)

# -------------------
# 3ï¸âƒ£ Multimodal Model (í›ˆë ¨ ì½”ë“œ(R20)ì™€ ë™ì¼)
# -------------------
class MultiModalBlinkModel(nn.Module):
    def __init__(self, img_dim=512, feature_dim=68*2 + 468*3): # 1540
        super().__init__()
        base_model = models.resnet18(pretrained=False) 
        base_model.fc = nn.Identity()
        self.cnn = base_model 

        self.feature_fc = nn.Sequential(
            nn.Linear(feature_dim, 512), # 1540 -> 512
            nn.ReLU(),
            nn.Dropout(0.3)
        )

        self.blink_fc = nn.Sequential(
            nn.Linear(512 + img_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 2)
        )

    def forward(self, img, features):
        img_feat = self.cnn(img)
        feat = self.feature_fc(features)
        combined = torch.cat((img_feat, feat), dim=1)
        blink_out = self.blink_fc(combined)
        return blink_out

# -------------------
# 4ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
# -------------------
feature_dim = 68*2 + 468*3 # 1540
model = MultiModalBlinkModel(feature_dim=feature_dim).to(DEVICE)
model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
model.eval() 
print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {SAVE_PATH}")

# -------------------
# 5ï¸âƒ£ Transform ì •ì˜
# -------------------
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) 
])

# -------------------
# 6ï¸âƒ£ ì›¹ìº  ì‹¤í–‰
# -------------------
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("ì˜¤ë¥˜: ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

blink_flag = False
blink_count = 0

print("ì›¹ìº  ì‹¤í–‰ ì¤‘... (ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤)")

while True:
    ret, frame = cap.read()
    if not ret:
        print("ì˜¤ë¥˜: í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break
    
    frame = cv2.flip(frame, 1) 
    
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    results = mp_face.process(rgb_frame)
    rects = detector(gray_frame)
    
    face_detected = False
    gaze_text = "N/A" # ì‹œì„  ê¸°ë³¸ê°’ (ë¬¸ìì—´ë¡œ ìœ ì§€)
    
    img_tensor = torch.zeros((1, 3, 224, 224), dtype=torch.float32).to(DEVICE)
    features_tensor = torch.zeros((1, feature_dim), dtype=torch.float32).to(DEVICE)
    
    if len(rects) > 0:
        face_detected = True
        shape = predictor(gray_frame, rects[0])
        
        # --- 1. ëª¨ë¸ ì…ë ¥ (ì´ë¯¸ì§€) & ëˆˆ ìœ„ì¹˜ ì‹œê°í™” (dlib ê¸°ë°˜) ---
        eye_coords_x = []
        eye_coords_y = []
        for i in range(36, 48):
            eye_coords_x.append(shape.part(i).x)
            eye_coords_y.append(shape.part(i).y)
        
        x_min = max(0, min(eye_coords_x) - EYE_CROP_PADDING)
        x_max = min(frame.shape[1], max(eye_coords_x) + EYE_CROP_PADDING)
        y_min = max(0, min(eye_coords_y) - EYE_CROP_PADDING)
        y_max = min(frame.shape[0], max(eye_coords_y) + EYE_CROP_PADDING)
        
        if x_max > x_min and y_max > y_min:
            eye_crop_img = frame[y_min:y_max, x_min:x_max]
            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (255, 255, 0), 2)
            rgb_crop = cv2.cvtColor(eye_crop_img, cv2.COLOR_BGR2RGB)
            img_tensor = transform(rgb_crop).unsqueeze(0).to(DEVICE)
        else:
            face_detected = False

        # --- 2. ëª¨ë¸ ì…ë ¥ (íŠ¹ì§•) ì¤€ë¹„ (dlib + MediaPipe) ---
        h, w, _ = frame.shape
        dlib_coords = []
        for p in shape.parts():
            dlib_coords.extend([p.x / w, p.y / h]) 
        dlib_f = np.array(dlib_coords)

        mp_f = np.zeros(468 * 3)
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0].landmark 
            mp_coords = []
            
            for i in range(468):
                lm = landmarks[i]
                mp_coords.extend([lm.x, lm.y, lm.z])
            mp_f = np.array(mp_coords)

            # --- ğŸ”´ 3. ì‹œì„  ì¶”ì • (MediaPipe ë™ê³µ) - ìˆ«ì ë§¤í•‘ìœ¼ë¡œ ìˆ˜ì • ---
            try:
                outer_corner_x = landmarks[33].x
                inner_corner_x = landmarks[133].x
                pupil_x = landmarks[473].x
                
                eye_width = abs(inner_corner_x - outer_corner_x)
                if eye_width > 0: 
                    # 0.0 ~ 1.0 ì‚¬ì´ì˜ ìƒëŒ€ ìœ„ì¹˜
                    pupil_pos = (pupil_x - outer_corner_x) / eye_width

                    # 0.0~1.0 ë²”ìœ„ë¥¼ -1.0~1.0 ë²”ìœ„ë¡œ ë§¤í•‘ (0.5ê°€ 0.0ì´ ë¨)
                    gaze_value = (pupil_pos - 0.5) * 2.0
                    
                    # (ì°¸ê³ ) í”„ë ˆì„ì´ ë°˜ì „ë˜ì—ˆìœ¼ë¯€ë¡œ,
                    # gaze_value < 0 -> ì‚¬ìš©ìê°€ ì™¼ìª½ì„ ë´„
                    # gaze_value > 0 -> ì‚¬ìš©ìê°€ ì˜¤ë¥¸ìª½ì„ ë´„
                    
                    gaze_text = f"{gaze_value:.2f}" # ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
                    
            except Exception as e:
                gaze_text = "Error"
                
        features = np.concatenate([dlib_f, mp_f]) 
        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(DEVICE)
        
    else:
        face_detected = False

    # --- 4. ëª¨ë¸ ì¶”ë¡  (ê¹œë¹¡ì„) ---
    pred = 1 # ê¸°ë³¸ê°’ 'Open'
    if face_detected:
        with torch.no_grad():
            blink_out = model(img_tensor, features_tensor)
            pred = torch.argmax(blink_out, dim=1).item() 
    else:
        gaze_text = "N/A" 

    # --- 5. ê¹œë¹¡ì„ ì¹´ìš´íŠ¸ ---
    if pred == 0: 
        if not blink_flag:
            blink_flag = True
    else: 
        if blink_flag:
            blink_flag = False
            blink_count += 1
            print(f"Blink! (Total: {blink_count})")

    # --- 6. í™”ë©´ í‘œì‹œ (ìˆ˜ì •ë¨) ---
    if not face_detected:
        status_text = "Face Not Detected"
        status_color = (0, 0, 255)
    else:
        status_text = "Closed" if pred == 0 else "Open"
        status_color = (0, 0, 255) if pred == 0 else (0, 255, 0)
    
    cv2.putText(frame, f"Status: {status_text}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)
    cv2.putText(frame, f"Blink Count: {blink_count}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)
    
    # ### ğŸ”´ ìˆ˜ì •: ì‹œì„  í…ìŠ¤íŠ¸ê°€ ìˆ«ìë¡œ í‘œì‹œë¨ ###
    if pred == 1 and face_detected: 
        cv2.putText(frame, f"Gaze: {gaze_text}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
    else:
        cv2.putText(frame, f"Gaze: N/A", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (128, 128, 128), 2)

    cv2.imshow("Blink Detection + Gaze (ESC to exit)", frame)

    if cv2.waitKey(1) & 0xFF == 27:  # ESC í‚¤
        break

# -------------------
# 7ï¸âƒ£ ì¢…ë£Œ
# -------------------
cap.release()
mp_face.close()
cv2.destroyAllWindows()
print(f"Final Blink Count: {blink_count}")
